## Предисловие

Этот файл содержит информацию о принципе работы трекера, а именно:
- последовательность этапов, которые выполняются от подключения к веб-камере и до получения искомых координат x,y на экране монитора
- детальное описание каждого этапа с ссылками на источники
- проблемы и трудности, с которыми можно столкнутся при доработке трекера

На момент написания документации к трекеру необходимо самостоятельно определять параметры камеры:
- вертикальное фокусное расстояние
- горизонтальное фокусное расстояние
- координаты основной точки (principle point)

Возможно данную информацию можно найти на просторах Интернета в характеристиках веб-камеры. Однако если камера встроена в ноутбук (или моноблок), то как правило такого рода информация нигде не публикуется. В таком случае нужно самостоятельно определить данные параметры с использованием определенных алгоритмов. Например, можно использовать алгоритм [шахматной доски](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html).

Дальнейшая документация предполагает, что у вас уже есть значения необходимых параметров камеры.

Все взаимодействие с трекером в рамках веб-приложения происходит через инстанс класса `FaceControls`, находящийся в глобальном объекте `window`. Класс `FaceControls` соединяет в себе все этапы работы трекера. В дальнешем будут более подробно описаны объекты других классов, предполагая, что все они используются инстансом `FaceControls`.

## Последовательность этапов

Процесс работы трекера состоит из следующих этапов:
1. Подключение к веб-камере и обработка видео-потока через MediaPipe
1. Нормализация изображения
1. Определение углов отклонения взгляда
1. Предсказание координат с применение регрессионной модели

## Подключение к веб-камере и обработка видео-потока через MediaPipe 

### Работа с веб-камерой

Для взаимодействия с веб-камерой устройства реализован класс `FaceCamera`, инкапсулирующий в себе функционал подключения к камере, получению видео-потока и корректного завершения работы с веб-камерой.

### MediaPipe

Обработка видео-потока с веб-камеры происходит с помощью класса `FaceTracker`. При инициализации данного класса происходит подключение MediaPipe в режиме `'video'`. При непосредственной обработке видео-потока получаемые от MediaPipe `landmarks` конвертируются из нормализованных координат (изначально координаты находятся в диапазоне от 0 до 1) в систему координат изображения.Это достигается путем умножения координаты `x` на ширину изображения, а `y` на высоту. Координата вдоль оси `z` при этом остается в нормализованном состоянии.

### Определение положения центров глаз

Для ряда последущих задач необходимо определить координаты центров глаз на изображении. Это достигается через тривиальное математическое среднее значение координат, составляющие область вокруг гла (верхнее и нижнее веко + внутренний и внешний угол). Данный функционал реализован в методе `getEyesCenterCoordinates` класса `FaceTracker`. 

### Определение расстояния между пользователем и монитором

Для определния расстояние между пользователем и камерой используется отношение между межзрачковым расстоянием на изображении в пикселях и реальным межзрачковым расстоянием (от 60 до 65 мм). Функционал реализован в методе `calculateDistanceBetweenCameraAndUser` класса `FaceTracker`.

## Нормализация изображения

Процесс нормализации изображения подробнее описани [здесь](https://www.collaborative-ai.org/publications/zhang18_etra.pdf).

В качестве внутренних параметров нормализованной камеры используются внутренние параметры исходной камеры.

Остановимся немного подробнее на основных этапах:

1. Определение системы координат головы.

Система координат головы происходит следующим образом: ось `x` строится от правого центра глаза к левому, ось `y` строится от точки между глаз до центра рта, ось `z` находится как векторное произведение полученных осей `x` и `y`.

Данная этап реализован в методе `getHeadAxis` класса `FaceTracker`

2. Определение системы координат нормализованной камеры.

Ось `z_c` нормализованной камеры определяется как вектор, идущий из начала координат системы координат камеры в сторону пользователя. Другими словами, если выразить ось `z_c` системы координат нормализованой камеры через систему координат исходной камеры (которая совпадает с системой координат изображения, но имеет трехмерную размерность, где ось `z` направлена в сторону пользователя), то получим вектор (0, 0, `userToCameraDistance`), где `userToCameraDistance` - [расстояние между пользователем](#определение-расстояния-между-пользователем-и-монитором) и камерой в миллиметрах. 

Определение оставшихся осей системы координат нормализованной камеры происходит через векторные произведения:

`y_c = x * z_c`, 
где `x` - ось x системы координат головы. 

В данном случае указанная формула верна, т.к. нам необходимо, чтобы система координат исходной камеры пришла в систему координат головы, т.е. чтобы ось x системы координат исходной камеры стала параллельна оси x системы координат головы.

Ось `x_c` нормализованной системы координат находится через векторное произведение двух векторов:
`x_c=y_c*z_c` 


Данный этап реализован в методе `getNormalizedCameraAxis` класса `FaceNormilizer`.

3. Определение матрицы перспективного преобразования

Из полученных векторов `x_c, y_c, z_c` определяем матрицу поворота `R = [x_c, y_c, z_c]`.

Для определения матрицы масштабирования `S` строится диагональная матрица:
`S = [0,0,norm_ditance/userToCameraDistane]`,

где `norm_ditance` - нормализированное расстояние между пользователем и камерой;
`userToCameraDistane` - [расстояние между пользователем и камерой](#определение-расстояния-между-пользователем-и-монитором) 

Расчет матрицы `W` происходит через умножение матриц внутренних параметров нормализированной камеры (аналогичны камере)

Данный этап реализован в методе `getWarpPerspectiveMatrix` класса `FaceNormilizer`.

4. Применение матрицы `W` к изображению

На данном этапе происходит преобразование изображения с веб-камеры с помощью матрицы `W` и обрезание области глаз. Таким образом достигается одинаковое расположение глаз на изображении вне зависимости от поворота головы пользователя.

Данный этап реализован в методе `normalizeImage` класса `FaceNormilizer`. Преобразование происходит с помощью библиотеки `opencv.js`.

## Определение углов отклонения взгляда

После нормализации изображение подается в самописную нейросетевую модель. [Нормализованное изображение](#нормализация-изображения) предобрабатывается в необходимый для модели формат. Весь функционал связанный с предсказанием углов определения инкапсулирован в классе `GazePredictor`.

### Нейросетевая модель

Определение углов отклонения взгляда происходит с помощью обученной нейросетевой модели на основе `EfficienNetB0`. Обучение происходило на датасете `MPIIGaze`. 

Изначально датасет содежит в `.mat` файлах изображения нормализованные по принципу используемом на [этапе 2](#нормализация-изображения) и дополнительно конвертированные в черно-белое изображение. Также трехмерные вектора определяющие направление взгляда на изображение. В процессе преобработки датасета был сформирован `.csv` файла содержащий метаинформацию о расположении файлов, а также конвертированные значения трехмерных векторов в два значения углов - `pitch` и `yaw` Подробнее о предобработки датасета можно узнать [тут](https://colab.research.google.com/drive/14AvHicd1N3RmowTmHb5ZxkLjg3sxRm7b?usp=drive_link).

Код обучения самой нейронной сети находится [тут](https://colab.research.google.com/drive/1Mit-c4xYkwQxKFKrrnf6kThQ-xC1gAx-?usp=drive_link).


### Подводные камни

#### Конвертирование модели

Как правило обучение модели производят в среде `python` из-за готовый инструментов, таких как `tensorflow` и возможностью использовать `google collab` для данной цели (что потенциально увеличивает скорость обучения модели в разы). Однако для того, чтобы использовать модель обученную на `python` в среде `TypeScript`, ее необходимо конвертировать в соответствующий формат. Для решения этой проблемы используется инструмент `tfjs-converter`.

В процессе конвертации может модели могут возникнуть следующий ряд сложностей:
- несовместимость версий `keras` у обученной модели и `tfjs-converter`.
Дело в том, что при использовании `tensorflow` в среде `google collab` по умолчанию используется `keras` третьей версии (`keras3`), однако под капотом `tfjs-converter` ожидает, что модель будет иметь формат соответствующий `keras` второй версии (`keras2`). При попытке конвертации несовместимых моделей как таковой ошибки не будет выдано, но `tfjs-converter` скажет, что не смог распознать структуру модели и конвертирует только ее веса.
Для избежания данной существует на пути решения:
    1. Изначально обучить модель на `keras2` (данный метод не подходил из-за сильного увеличения времени обучения модели).
    1. Обучить модель на `keras3`, сохранить ее в файл, проинициализировать аналогичную модель на `keras2` и загрузить модель из файла, содержающую в себе модель `keras3` (этот способ выбран в текущей реализации трекера).

- некорректное конвертирование слоев модели в `model.js`.
Если модель имеет необходимый формат для `tfjs-converter`, в процессе ее конвертации может быть нарушена структура некоторых слоев.
В контексте используемой модели на основе `EfficienNetB0`, в процессе конвертации ломается слой `TFOpLambda`.

После конвертации модели в `model.js` в слое `TFOpLambda` поле `inbound_nodes` принудительно требуется обернуть в дополнительный массив. Если этого не сделать, то при загрузки модели приложение уйдет в бесконечный цикл внутри пакет `tensorflow.js` и вкладка повиснет без возможности восстановления. Обусловленно ошибкой либо у конвертора `tensorflowjs_converter`, либо ошибкой самого `tensorflow.js`. Issue на GitHub существует, но решать никто пока не собирается: https://github.com/tensorflow/tfjs/issues/7271

Также для корректной работы модели в текущей реализации необходимо в файле `model.js` у слоя `TFOpLambda` самостоятельно скопировать объект `y` из поля `inbound_nodes` в поле `config`. Подробнее см. [тут](#загрузка-модели)

Пример как выдает `tfjs-converter` после конвертации модели `keras2` в модель `tensorflow.js`:
```json
{
    "class_name": "TFOpLambda",
    "config": {
        "name": "tf.math.multiply",
        "trainable": true,
        "dtype": "float32",
        "function": "math.multiply"
    },
    "name": "tf.math.multiply",
    "inbound_nodes": [
        [
            "normalization",
            0,
            0,
            {
                "y": [
                    [
                        [
                            [
                                2.0896918773651123,
                                2.1128857135772705,
                                2.108185052871704
                            ]
                        ]
                    ]
                ]
            }
        ]
    ]
}
```

Как должно быть:
```json
{
    "class_name": "TFOpLambda",
    "config": {
        "name": "tf.math.multiply",
        "trainable": true,
        "dtype": "float32",
        "function": "math.multiply",
        "y": [
            [
                [
                    [
                        2.0896918773651123,
                        2.1128857135772705,
                        2.108185052871704
                    ]
                ]
            ]
        ]
    },
    "name": "tf.math.multiply",
    "inbound_nodes": [
        [
            [
                "normalization",
                0,
                0,
                {
                    "y": [
                        [
                            [
                                [
                                    2.0896918773651123,
                                    2.1128857135772705,
                                    2.108185052871704
                                ]
                            ]
                        ]
                    ]
                }
            ]
        ]
    ]
}
```

#### Загрузка модели

Загрузка модели происходит в методе `init` класса `GazePredictor`.

При загрузке модели могут быть обнаружены слеудющие проблемы:
- зависание при попытке загрузить модель. Подробнее см. [тут](#конвертирование-модели).
- ошибка загрузки некоторых слоев.
Данная ошибка обусловлена отсутствием данных слоев в api `tensorflow.js`. Для ее решения необходимо самостоятельно реализовать функциональность отсутствующих слоев и зарегистрировать данные слои в api `tensorflow.js`.
В контексте текущей реализации имеют место 2 самописных слоя: `Normalization` (слой `keras.layers.Normalization` в `python`) и `TFOpLambda` (слой `tensorflow.python.keras.layers.core.TFOpLambda`) работающий с операцией `tf.multiply`.

Перед запуском модели требуется убедится, что в описании слоя `TFOpLambda` в `model.json` параметр `y` продублирован из `inbound_nodes`, в поле `config`. Если поле `y` будет отсутствовать в поле `config`, то в консоль выведется соответствующая ошибка и модель не запустится. Данный костыль обусловлен тем, что `tensorflow.js` не предоставляет возможности взаимодействовать с полем `inbound_nodes` напрямую.


## Предсказание координат с применение регрессионной модели

Для предсказания координат на экране монитора используется полиномиальная регрессионная модель второй степени (подробнее смотри класс `PolynomialRegressionModel`). При первом запуске трекинга взгляда пользователю необходимо ее обучить. При этом происходит переход на `TraningPage`. Обучение состоит из следующих этапов:
1. Клики на статичные маркеры
1. Слежение за движущимся маркером. 
В данной реализации из-за долгой обработки изображения нейросетью данный движение маркера занимает большое количество времени (около 5 минут). В противном случае маркер двигался бы дергано из-за зависаний на этапе обработке изображения. В качестве оптимизации можно предложить вынести логику по получения характеристик взгляда (метод `getGazeFeatures`объекта `FaceControls`) в отдельный `worker`.
1. Клики по статичным точкам в повернутой головой вправо
1. Клики по статичным точка с повернутой головой влево
1. Дополнительный этап обучения. Данный этап включает в себя бесконечную мини игру в которой пользователю необходимо следить за курсором мыши.

Отдельно стоит отметить, что при слежении за движущимся маркером велика вероятность собрать большое количество шумных данных, что в последствии приведет к ужасной точности самой модели. Вероятно в данной версии работы трекера от данного этапа вообще лучше отказаться, отдав предпочтение небольшому, но качественному сбору информации по статическим маркерам.

После обучения модели весовые коэффициенты сохраняются в `localStorage`. После предсказания регрессионной моделью координат на экране монитора они обрабатываются с помощью фильтра Калмана для снижения шумности данных. 

## Послесловие

В процессе разработки новой версии трекера оставлен класс `FaceScreen`, использовавшийся в предыдущей версии. В текущей реализации он не используется, но с высокой долей вероятности учитывать `viewport` может быть полезным (из-за высокой сложности системы, сжатых сроков и т.д. разработчик не успел до конца разобраться с данным функционалом и предпочел оставить его нетронутым).

В качестве улучшения текущей модели предсказания углов отклонения взгляда вероятно можно использовать более легкие исходные модели. Из-за большого количества задействованных вычислительных ресурсов на работу с изображениями и работу с нейронной сетью трекер реагирует на движения пользователя с заметной задержкой. Возможно это обусловленно также и аппаратными средствами на котором тестировался трекер (встроенное графическое ядро ноутбука, отсутствие какой-либо графической карты).

В случайный промежуток времени может возникнуть ситуация из-за которой трекер останавливается. Ошибка обусловленна внутренним устройством `tensorflow.js`, а именно использование `WebGL`. В случайный момент времени библиотека начинает ругаться на дублирование контекста и после непродолжительного времени отказывается работать и требуется перезагрузки страницы.

В целом общая точность системы обуславливается множеством факторов: степень освещения, наличием очков у пользователя, степень обученности регрессионной модели. Последний фактор как правило имеет большее значение, т.к. для того, чтобы пользователь мог менять свое положение с сохранением точности трекера, необходимо, чтобы на этапе обучения данные положения пользователя также учитывались.